# -*- coding: utf-8 -*-
"""
Automatically generated by Colaboratory.

Author: Kanak Tenguria
"""
import os
import random
import pickle
import numpy as np
import tensorflow as tf
from tensorflow import keras
import matplotlib.pyplot as plot
from sklearn.metrics import classification_report, confusion_matrix

def load(file_name):
    with open(file_name, 'rb') as fp:
        obj = pickle.load(fp)
    return obj

# # Load data for training and testing
X_train = load('DataFiles/X_train.pck')
y_train = load('DataFiles/y_train.pck')
X_test = load('DataFiles/X_test.pck')
y_test = load('DataFiles/y_test.pck')
X_valid = load('DataFiles/X_valid.pck')
y_valid = load('DataFiles/y_valid.pck')
pred_image = load('DataFiles/pred_image.pck')

# # Normalize data for faster calculation
X_train = X_train/255
X_test = X_test/255
X_valid = X_valid/255
pred_image = pred_image/255

# Define a simple sequential model
def create_model():
  modelANN = tf.keras.models.Sequential()

  modelANN.add(keras.layers.Flatten(input_shape = (64,64,3)))
  modelANN.add(keras.layers.BatchNormalization())

  modelANN.add(keras.layers.Dense(2048, activation='relu'))
  modelANN.add(keras.layers.Dense(1024, activation='relu'))
  modelANN.add(keras.layers.Dropout(rate=0.5))

  modelANN.add(keras.layers.Dense(1024, activation='relu'))
  modelANN.add(keras.layers.Dense(512, activation='relu'))
  modelANN.add(keras.layers.Dropout(rate=0.5))

  modelANN.add(keras.layers.Dense(128, activation='relu'))
  modelANN.add(keras.layers.BatchNormalization())

  modelANN.add(keras.layers.Dense(64, activation='relu'))
  modelANN.add(keras.layers.Dropout(rate=0.5))

  modelANN.add(keras.layers.Dense(6, activation='softmax'))
  modelANN.compile(optimizer=keras.optimizers.Adam(0.00001),
                loss=tf.losses.SparseCategoricalCrossentropy(from_logits=True),
                metrics=[tf.metrics.SparseCategoricalAccuracy()])

  return modelANN

def getLabel(class_index):
    labels = {0: 'buildings', 1: 'forest', 2: 'glacier', 3: 'mountain', 4: 'sea', 5: 'street'}
    return labels[class_index]

'''Training'''
# # Create a basic model instance
# model = create_model()
#
# # Display the model's architecture
# model.summary()
#
# # Save location
# checkpoint_path = "ANN_Model/ANN_Model.ckpt"
# checkpoint_dir = os.path.dirname(checkpoint_path)
#
# batchsize = 64
# epoch = 23
#
# # Create a callback that saves the model's weights
# cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath = checkpoint_path,
#                                                  save_weights_only = True,
#                                                  verbose = 0)
#
# # Train the model with the new callback
# trained = model.fit(X_train,
#           y_train,
#           epochs = epoch,
#           batch_size = batchsize,
#           validation_data = (X_valid, y_valid),
#           callbacks = [cp_callback])  # Remove "callbacks= [cp_callback]" to stop saving weights
#
# # Plot accuracy and loss (Training v/s Validation)
# plot.plot(trained.history['sparse_categorical_accuracy'])
# plot.plot(trained.history['val_sparse_categorical_accuracy'])
# plot.title('Model accuracy')
# plot.ylabel('Accuracy')
# plot.xlabel('Epoch')
# plot.legend(['Train', 'Validate'], loc='upper left')
# plot.show()
#
# plot.plot(trained.history['loss'])
# plot.plot(trained.history['val_loss'])
# plot.title('Model loss')
# plot.ylabel('Loss')
# plot.xlabel('Epoch')
# plot.legend(['Train', 'Validate'], loc='upper left')
# plot.show()
#
# # Evaluate Model
# model.evaluate(X_test,y_test,verbose=1)

'''Testing'''

# Create a basic model instance
newmodel = create_model()

# Loads the weights
checkpoint_path = "ANN_Model/ANN_Model.ckpt"
checkpoint_dir = os.path.dirname(checkpoint_path)
newmodel.load_weights(checkpoint_path)

# # Evaluate the model on test data -------------------------
loss, acc = newmodel.evaluate(X_test, y_test, verbose=1)
print("Restored model, Test accuracy: {:5.2f}%".format(100 * acc))

y_pred_test = np.argmax(newmodel.predict(X_test), axis=1)

print('Test Classification report')
print(classification_report(y_pred_test, y_test))

print('Test Confusion report')
print(confusion_matrix(y_pred_test, y_test))
# # -----------------------------------------------------------

# # Prediction on un-labelled data ----------------------------
y_pred_test1 = np.argmax(newmodel.predict(pred_image), axis=1)

f,ax = plot.subplots(3,3)
for i in range(0,3,1):
    for j in range(0,3,1):
        rnd_number = random.randint(0, len(pred_image))
        ax[i,j].imshow(pred_image[rnd_number])
        ax[i,j].set_title(getLabel(y_pred_test1[rnd_number]))
        ax[i,j].axis('off')
plot.show()
# # -----------------------------------------------------------